{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b53d2826-a069-4957-877a-4063e5ece03f",
   "metadata": {},
   "source": [
    "# Vision transformer\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "### Libraries and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "072e5e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d626a773-6963-40b2-b356-17072a9b38f6",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b003b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST images are 28x28\n",
    "IMAGE_SIZE = 28\n",
    "\n",
    "# Divide image into (28/7)x(28/7) patches\n",
    "PATCH_SIZE = 7\n",
    "NUM_SPLITS = IMAGE_SIZE // PATCH_SIZE\n",
    "NUM_PATCHES = NUM_SPLITS ** 2\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "EMBEDDING_DIM = 8\n",
    "NUM_HEADS = 2\n",
    "NUM_CLASSES = 10\n",
    "NUM_TRANSFORMER_LAYERS = 4\n",
    "HIDDEN_DIM = 16\n",
    "EPOCHS = 3\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110272da-00ff-452f-bd4b-9bdf0a98342d",
   "metadata": {},
   "source": [
    "## The `MNIST` dataset\n",
    "\n",
    "See [here](https://en.wikipedia.org/wiki/MNIST_database) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c451010",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./.data\", train=True, transform=transform, download=True\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./.data\", train=False, transform=transform, download=True\n",
    ")\n",
    "\n",
    "# Define data loader with `BATCH_SIZE` and shuffle\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6351d30-cf79-4a85-8dad-8e50fa529f5b",
   "metadata": {},
   "source": [
    "## Patch Embedding Layer\n",
    "\n",
    "The first module to implement is a module that will transformed a tensor\n",
    "of size `BATCH_SIZE` \\* 1 \\* `IMAGE_SIZE` \\* `IMAGE_SIZE` into a tensor\n",
    "of size `BATCH_SIZE` \\* `NUM_PATCHES` \\* `EMBEDDING_DIM`. This can be\n",
    "done by using a `nn.Conv2d` module with both the stride and the kernel\n",
    "the size of a patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2614296",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels=1, patch_size=7, embedding_dim=64):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # Use `nn.Conv2d` to split the image into patches\n",
    "        self.projection = nn.Conv2d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # `x` is `BATCH_SIZE` * 1 * `IMAGE_SIZE` * `IMAGE_SIZE`\n",
    "\n",
    "        # Project `x` into a tensor of size `BATCH_SIZE` * `EMBEDDING_DIM` *\n",
    "        # `NUM_SPLITS` * `NUM_SPLITS`\n",
    "        x = self.projection(x)\n",
    "\n",
    "        # Flatten spatial dimensions to have a tensor of size `BATCH_SIZE` *\n",
    "        # `EMBEDDING_DIM` * `NUM_PATCHES`\n",
    "        x = x.flatten(2)\n",
    "\n",
    "        # Put the `NUM_PATCHES` dimension at the second place to have a tensor\n",
    "        # of size `BATCH_SIZE` * `NUM_PATCHES`` * `EMBEDDING_DIM`\n",
    "        x = x.permute([0, 2, 1])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488b0566-86f5-4cf5-a998-8260c292aae7",
   "metadata": {},
   "source": [
    "## Transformer encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "873132d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, hidden_dim):\n",
    "        super().__init__()\n",
    "        # Define a `nn.MultiheadAttention` module with `embedding_dim` and\n",
    "        # `num_heads`. Don't forget to set `batch_first` to `True`\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        # Define the position-wise feed-forward network using an `nn.Sequential`\n",
    "        # module, which consists of a linear layer, a GELU activation function,\n",
    "        # and another linear layer\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "        # Define two layer normalization modules\n",
    "        self.layernorm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute self-attention on `x`\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "\n",
    "        # Skip-connection and first layer normalization\n",
    "        x = self.layernorm1(x + attn_output)\n",
    "\n",
    "        # Apply the position-wise feed-forward network\n",
    "        mlp_output = self.mlp(x)\n",
    "\n",
    "        # Skip-connection and second layer normalization\n",
    "        x = self.layernorm2(x + mlp_output)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70978c8-6984-4643-a1ed-f90248882a9d",
   "metadata": {},
   "source": [
    "## Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2452670",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            patch_size,\n",
    "            embedding_dim,\n",
    "            num_heads,\n",
    "            num_classes,\n",
    "            num_transformer_layers,\n",
    "            hidden_dim,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define a `PatchEmbedding` module\n",
    "        self.patch_embedding = PatchEmbedding(in_channels=1, patch_size=patch_size, embedding_dim=embedding_dim)\n",
    "\n",
    "        # Use `nn.Parameter` to define an additional token embedding that will\n",
    "        # be used to predict the class\n",
    "        self.cls_token = nn.Parameter(torch.zeros((1, 1, embedding_dim)))\n",
    "\n",
    "        # Use `nn.Parameter` to define a learnable positional encoding.\n",
    "        self.position_embedding = nn.Parameter(torch.randn(1, NUM_PATCHES + 1, embedding_dim))\n",
    "\n",
    "        # Use `nn.init.xavier_uniform_` to initialize the positional embedding\n",
    "        nn.init.xavier_uniform_(self.position_embedding)\n",
    "\n",
    "        self.encoder_layers = nn.Sequential(\n",
    "            *[\n",
    "                TransformerEncoder(embedding_dim, num_heads, hidden_dim)\n",
    "                for _ in range(num_transformer_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embedding_dim), nn.Linear(embedding_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # `x` is `BATCH_SIZE` * 1 * `IMAGE_SIZE` * `IMAGE_SIZE`\n",
    "\n",
    "        # Transform images into embedded patches. It gives a tensor of size\n",
    "        # `BATCH_SIZE` * `NUM_PATCHES` * `EMBEDDING_DIM`\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        # We need to add the embedded classification token at the beginning of\n",
    "        # each sequence in the minibatch. Use `expand` to duplicate it along the\n",
    "        # batch size dimension\n",
    "        batch_size = x.shape[0]\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "\n",
    "        # Next use `torch.cat` to concatenate `cls_tokens` and `x` to have a\n",
    "        # tensor of size `BATCH_SIZE` * (NUM_PATCHES + 1) * `EMBEDDING_DIM`\n",
    "        x = torch.cat((cls_tokens, x), dim = 1)\n",
    "\n",
    "        # Add the positional encoding\n",
    "        x += self.position_embedding\n",
    "\n",
    "        # Apply the stacked transformer modules\n",
    "        y = self.encoder_layers(x)\n",
    "\n",
    "        # Select the classification token for each sample in the minibatch.\n",
    "        # `cls_output` should be of size `BATCH_SIZE` * 1 * `EMBEDDING_DIM`\n",
    "        cls_output = y[:, 0, :]\n",
    "\n",
    "        # Use `self.mlp_head` to adapt the output size to NUM_CLASSES.\n",
    "        out = self.mlp_head(cls_output)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51acf46-b8b5-40ca-885d-07f631afed52",
   "metadata": {},
   "source": [
    "## Initialize model, loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "174fa3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the `VisionTransformer` model\n",
    "model = VisionTransformer(PATCH_SIZE, EMBEDDING_DIM, NUM_HEADS, NUM_CLASSES, NUM_TRANSFORMER_LAYERS, HIDDEN_DIM)\n",
    "\n",
    "# Use cross-entropy loss and AdamW optimizer with a learning rate of 5e-3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26e5def-2cb0-40cf-bb75-663d80f61b3c",
   "metadata": {},
   "source": [
    "## Validation loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b74010ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    total, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return val_loss / len(val_loader), accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e6fa93-b07e-47f8-88ed-bb52446ed197",
   "metadata": {},
   "source": [
    "## Training with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eaa32fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Train Loss: 0.9254\n",
      "Val Loss: 0.4883, Val Accuracy: 84.69%\n",
      "Epoch 2/3\n",
      "Train Loss: 0.3961\n",
      "Val Loss: 0.3304, Val Accuracy: 90.47%\n",
      "Epoch 3/3\n",
      "Train Loss: 0.3058\n",
      "Val Loss: 0.2583, Val Accuracy: 92.40%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Calculate validation loss and accuracy\n",
    "    val_loss, val_accuracy = validate_model(model, test_loader, criterion)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
