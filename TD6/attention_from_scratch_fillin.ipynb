{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f232a4-f30e-474d-9cdc-56c5b2947a71",
   "metadata": {},
   "source": [
    "# The transformer architecture from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "faa6edcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections.abc import Iterable\n",
    "from timeit import default_timer as timer\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576d00c9-375d-45cb-910d-b205498cd09a",
   "metadata": {},
   "source": [
    "## Toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38cec014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_deterministic(input_sequence):\n",
    "    target_sequence = []\n",
    "    for i, elt in enumerate(input_sequence):\n",
    "        try:\n",
    "            offset = int(elt)\n",
    "        except ValueError:  # It is a letter\n",
    "            target_sequence.append(elt)\n",
    "        else:               # Special token, do the lookup\n",
    "            if i + offset < 0 or i + offset > len(input_sequence) - 1:\n",
    "                pass\n",
    "            else:\n",
    "                k = min(max(0, i + offset), len(input_sequence) - 1)\n",
    "                target_sequence.append(input_sequence[k])\n",
    "\n",
    "    return target_sequence\n",
    "\n",
    "\n",
    "class GotoDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seed=None,\n",
    "        n_sequences=100,\n",
    "        min_length=4,\n",
    "        max_length=20,\n",
    "        n_letters=3,\n",
    "        offsets=[4, 5, 6],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        full_vocab = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "        full_vocab = list(full_vocab.upper()) + list(full_vocab)\n",
    "        assert(n_letters <= len(full_vocab))\n",
    "\n",
    "        self.vocab = np.array(\n",
    "            [s + str(d) for s in [\"+\", \"-\"] for d in offsets] + full_vocab[:n_letters]\n",
    "        )\n",
    "        self.n_tokens = len(self.vocab)\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "        self.seed = seed\n",
    "        self.n_sequences = n_sequences\n",
    "\n",
    "        # Dataset generation\n",
    "        rs = np.random.RandomState(self.seed)\n",
    "        seq_lengths = rs.randint(\n",
    "            self.min_length, self.max_length, size=self.n_sequences\n",
    "        )\n",
    "        self.input_sequences = [\n",
    "            list(self.vocab[rs.randint(self.n_tokens, size=seq_length)])\n",
    "            for seq_length in seq_lengths\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_sequences\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        input_sequence = self.input_sequences[i]\n",
    "        target_sequence = translate_deterministic(input_sequence)\n",
    "        return input_sequence, target_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d33f9a-39ef-415b-abed-934976eaefe0",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "179a0a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GotoDataset()\n",
    "special_tokens = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "idx2tok = special_tokens + dataset.vocab.tolist()\n",
    "tok2idx = {token: i for i, token in enumerate(idx2tok)}\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = [tok2idx[tok] for tok in special_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6736b2c0-12b9-4fcb-929d-176431365beb",
   "metadata": {},
   "source": [
    "## Collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87649caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: List):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "\n",
    "        # Numericalize list of tokens using `vocab`.\n",
    "        #\n",
    "        # - Don't forget to add beginning of sequence and end of sequence tokens\n",
    "        #   before numericalizing.\n",
    "        #\n",
    "        # - Use `torch.LongTensor` instead of `torch.Tensor` because the next\n",
    "        #   step is an embedding that needs integers for its lookup table.\n",
    "        # <answer>\n",
    "        src_tensor = torch.LongTensor([tok2idx[tok] for tok in [\"<bos>\"] + src_sample + [\"<eos>\"]])\n",
    "        tgt_tensor = torch.LongTensor([tok2idx[tok] for tok in [\"<bos>\"] + tgt_sample + [\"<eos>\"]])\n",
    "        # </answer>\n",
    "\n",
    "        # Append numericalized sequence to `src_batch` and `tgt_batch`\n",
    "        src_batch.append(src_tensor)\n",
    "        tgt_batch.append(tgt_tensor)\n",
    "\n",
    "    # Turn `src_batch` and `tgt_batch` that are lists of 1-dimensional\n",
    "    # tensors of varying sizes into tensors with same size with\n",
    "    # padding. Use `pad_sequence` with padding value to do so.\n",
    "    #\n",
    "    # Important notice: by default resulting tensors are of size\n",
    "    # `max_seq_length` * `batch_size`; the mini-batch size is on the\n",
    "    # *second dimension*.\n",
    "    # <answer>\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    # </answer>\n",
    "\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cfc461-9c74-4805-82cc-591c7ecc290f",
   "metadata": {},
   "source": [
    "## Hyperparameters of transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "696f0516",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Size of source and target vocabulary\n",
    "VOCAB_SIZE = len(idx2tok)\n",
    "\n",
    "# Number of sequences generated for the training set\n",
    "N_SEQUENCES = 7000\n",
    "\n",
    "# Number of epochs\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# Size of embeddings\n",
    "EMB_SIZE = 64\n",
    "\n",
    "# Number of heads for the multihead attention\n",
    "NHEAD = 1\n",
    "\n",
    "# Size of hidden layer of FFN\n",
    "FFN_HID_DIM = 128\n",
    "\n",
    "# Size of mini-batches\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Number of stacked encoder modules\n",
    "NUM_ENCODER_LAYERS = 1\n",
    "\n",
    "# Number of stacked decoder modules\n",
    "NUM_DECODER_LAYERS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1511375a-10af-41e3-bdb7-e87c399558f0",
   "metadata": {},
   "source": [
    "## Transformer encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21a5662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout: float = 0.1, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Define Tk/2pi for even k between 0 and `emb_size`. Use\n",
    "        # `torch.arange`.\n",
    "        # <answer>\n",
    "        Tk_over_2pi = 10000 ** (torch.arange(0, emb_size, 2) / emb_size)\n",
    "        # </answer>\n",
    "\n",
    "        # Define `t = 0, 1,..., maxlen-1`. Use `torch.arange`.\n",
    "        # <answer>\n",
    "        t = torch.arange(maxlen)\n",
    "        # </answer>\n",
    "\n",
    "        # Outer product between `t` and `1/Tk_over_2pi` to have a\n",
    "        # matrix of size `maxlen` * `emb_size // 2`. Use\n",
    "        # `torch.outer`.\n",
    "        # <answer>\n",
    "        outer = torch.outer(t, 1 / Tk_over_2pi)\n",
    "        # </answer>\n",
    "\n",
    "        pos_embedding = torch.empty((maxlen, emb_size))\n",
    "\n",
    "        # Fill `pos_embedding` with either sine or cosine of `outer`.\n",
    "        # <answer>\n",
    "        pos_embedding[:, 0::2] = torch.sin(outer)\n",
    "        pos_embedding[:, 1::2] = torch.cos(outer)\n",
    "        # </answer>\n",
    "\n",
    "        # Add fake mini-batch dimension to be able to use broadcasting\n",
    "        # in `forward` method.\n",
    "        pos_embedding = pos_embedding.unsqueeze(1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Save `pos_embedding` when serializing the model even if it is not a\n",
    "        # set of parameters\n",
    "        self.register_buffer(\"pos_embedding\", pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        # `token_embedding` is of size `seq_length` * `batch_size` *\n",
    "        # `embedding_size`. Use broadcasting to add the positional embedding\n",
    "        # that is of size `seq_length` * 1 * `embedding_size`.\n",
    "        # <answer>\n",
    "        seq_length = token_embedding.size(0)\n",
    "        positional_encoding = token_embedding + self.pos_embedding[:seq_length, :]\n",
    "        # </answer>\n",
    "\n",
    "        return self.dropout(positional_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a774a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            p=None,                  # Embedding size of input tokens\n",
    "            d_ff=None,               # Size of hidden layer in MLP\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Size of embedding. Here sizes of embedding, keys, queries\n",
    "        # and values are the same.\n",
    "        self.p = p\n",
    "        d_q = d_v = d_k = p\n",
    "\n",
    "        # Size of hidden layer in MLP\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        # Compute query, key and value from input\n",
    "        self.enc_Q = nn.Linear(p, d_q)\n",
    "        self.enc_K = nn.Linear(p, d_k)\n",
    "        self.enc_V = nn.Linear(p, d_v)\n",
    "\n",
    "        # Linear transform just before first residual mapping\n",
    "        self.enc_W0 = nn.Linear(d_v, p)\n",
    "\n",
    "        # Layer normalization after first residual mapping\n",
    "        self.enc_ln1 = nn.LayerNorm(p)\n",
    "\n",
    "        # Position-wise MLP\n",
    "        self.enc_W1 = nn.Linear(p, d_ff)\n",
    "        self.enc_W2 = nn.Linear(d_ff, p)\n",
    "\n",
    "        # Final layer normalization of second residual mapping\n",
    "        self.enc_ln2 = nn.LayerNorm(p)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward propagation in encoder. Input tensor `X` is of size\n",
    "        # `seq_length` * `batch_size` * `p`.\n",
    "\n",
    "        # Query, key and value of the encoder. Use `enc_Q`, `enc_K`\n",
    "        # and `enc_V`.\n",
    "        Q = self.enc_Q(X)\n",
    "        K = self.enc_K(X)\n",
    "        V = self.enc_V(X)\n",
    "\n",
    "        # Score attention from `Q` and `K`. We need to compute `QK^T` but both\n",
    "        # `Q` and `K` are not just simple matrices but batch of matrices. Both\n",
    "        # `Q` and `K` are in fact of size `seq_length` * `batch_size` *\n",
    "        # `emb_size`. Two ways to compute the batched matrix product:\n",
    "        #\n",
    "        # - permute dimensions using `torch.permute` so that `batch_size` is the\n",
    "        #   first dimension and use `torch.bmm` that will perform the batch\n",
    "        #   matrix product with respect to the first dimension,\n",
    "        # - use `torch.einsum` to specify the product.\n",
    "        # S = torch.bmm(Q.permute([1, 0, 2]), K.permute([1, 2, 0])) / math.sqrt(self.p)\n",
    "        S = torch.einsum('sbe, Sbe->bsS', Q, K) / math.sqrt(self.p)\n",
    "\n",
    "        # Compute attention from `S` and `V`. You can use `F.softmax` with `dim`\n",
    "        # argument. Since the mini-batch dimension is now the first one for `S`\n",
    "        # we can use `torch.bmm` with `S` (after softmax). That is not the case\n",
    "        # for `V` so we need to transpose it first. Don't forget to transpose\n",
    "        # again after the product to have a matrix `seq_length` * `batch_size` *\n",
    "        # `emb_size` compatible with `X` for the residual mapping.\n",
    "        A = F.softmax(S, dim = -1)\n",
    "        #T = torch.bmm(A, V.transpose(0, 1)).transpose(0, 1)\n",
    "        T = torch.einsum(\"bsS,Sbe->sbe\", A, V)\n",
    "        \n",
    "        # First residual mapping and layer normalization\n",
    "        U = self.enc_ln1(self.enc_W0(T) + X)\n",
    "\n",
    "        # FFN on each token\n",
    "        Z = self.enc_W2(F.relu(self.enc_W1(U)))\n",
    "\n",
    "        # Second residual mapping and layer normalization\n",
    "        Xp = self.enc_ln2(Z + U)\n",
    "\n",
    "        return Xp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d61644d-6c46-44dd-8f66-0c82e8ae132c",
   "metadata": {},
   "source": [
    "## Transformer decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "361be54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            p=None,                  # Embedding size of input tokens\n",
    "            d_ff=None,               # Size of hidden layer in MLP\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Size of embedding. Here, sizes of embedding, keys, queries\n",
    "        # and values are the same.\n",
    "        self.p = p\n",
    "        self.d_q = self.d_v = self.d_k = p\n",
    "\n",
    "        # Size of hidden layer in MLP\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        # Compute query, key and value from input\n",
    "        self.dec_Q1 = nn.Linear(p, self.d_q)\n",
    "        self.dec_K1 = nn.Linear(p, self.d_k)\n",
    "        self.dec_V1 = nn.Linear(p, self.d_v)\n",
    "\n",
    "        # Linear transform just before first residual mapping\n",
    "        self.dec_W0 = nn.Linear(self.d_v, p)\n",
    "\n",
    "        # Layer normalization after first residual mapping\n",
    "        self.dec_ln1 = nn.LayerNorm(p)\n",
    "\n",
    "        # Key-value cross-attention\n",
    "        self.dec_Q2 = nn.Linear(p, self.d_k)\n",
    "        self.dec_K2 = nn.Linear(p, self.d_k)\n",
    "        self.dec_V2 = nn.Linear(p, self.d_v)\n",
    "\n",
    "        # Linear transform just before first residual mapping\n",
    "        self.dec_W1 = nn.Linear(self.d_v, p)\n",
    "\n",
    "        # Layer normalization after second residual mapping\n",
    "        self.dec_ln2 = nn.LayerNorm(p)\n",
    "\n",
    "        # Position-wise MLP\n",
    "        self.dec_W2 = nn.Linear(p, d_ff)\n",
    "        self.dec_W3 = nn.Linear(d_ff, p)\n",
    "\n",
    "        # Final layer normalization of second residual mapping\n",
    "        self.dec_ln3 = nn.LayerNorm(p)\n",
    "\n",
    "    def forward(self, Xp, Y):\n",
    "        # Forward propagation in decoder. Input tensor `Xp` is of size\n",
    "        # `seq_length_src` * `batch_size` * `p` and `Y` is of size\n",
    "        # `seq_length_tgt` * `batch_size` * `p`.\n",
    "\n",
    "\n",
    "        # Set number of tokens in target sequence `Y`. Needed to\n",
    "        # compute the mask.\n",
    "        m = Y.size(0)\n",
    "\n",
    "        # Forward propagation of decoder. Use `dec_Q1`, `dec_K` and\n",
    "        # `dec_V`.\n",
    "        Q = self.dec_Q1(Y)\n",
    "        K = self.dec_K1(Y)\n",
    "        V = self.dec_V1(Y)\n",
    "\n",
    "        # Compute square upper triangular mask matrix of size `m`. You\n",
    "        # can use `torch.triu` and `torch.full` with `float(\"-inf\")`.\n",
    "        M = torch.triu(torch.full((m, m), float(\"-inf\")), diagonal=1)\n",
    "\n",
    "        # Score attention from `Q` and `K`. You can use `torch.bmm`\n",
    "        # and `transpose` but don't forget to add the mask `M`.\n",
    "        S = (torch.bmm(Q.permute([1, 0, 2]), K.permute([1, 2, 0])) + M) / math.sqrt(self.p)\n",
    "\n",
    "        # Attention\n",
    "        A = F.softmax(S, dim = -1)\n",
    "        T1 = torch.bmm(A, V.transpose(0, 1)).transpose(0, 1)\n",
    "\n",
    "        # First residual mapping and layer normalization\n",
    "        U1 = self.dec_ln1(self.dec_W0(T1) + Y)\n",
    "\n",
    "        # Key-value cross-attention using keys and values from the\n",
    "        # encoder.\n",
    "        Q = self.dec_Q2(U1)\n",
    "        K = self.dec_K2(Xp)\n",
    "        V = self.dec_V2(Xp)\n",
    "\n",
    "        # Score attention from `Q` and `K`. You can either use\n",
    "        # `torch.bmm` together with `torch.permute` or `torch.einsum`.\n",
    "        # S = torch.bmm(Q.permute([1, 0, 2]), K.permute([1, 0, 2])) / math.sqrt(self.p)\n",
    "        S = torch.einsum('sbe, Sbe->bsS', Q, K) / math.sqrt(self.p)\n",
    "\n",
    "        # Attention\n",
    "        A = F.softmax(S, dim = 1)\n",
    "        T2 = torch.bmm(A, V.transpose(0, 1)).transpose(0, 1)\n",
    "\n",
    "        # Second residual mapping and layer normalization\n",
    "        U2 = self.dec_ln2(self.dec_W1(T2) + U1)\n",
    "\n",
    "        # FFN on each token\n",
    "        Z = self.dec_W3(F.relu(self.dec_W2(U2)))\n",
    "\n",
    "        # Third residual mapping and layer normalization\n",
    "        U3 = self.dec_ln3(Z + U2)\n",
    "\n",
    "        return U3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aadfdd-ffa2-4608-8871-8768c4dea3d8",
   "metadata": {},
   "source": [
    "## Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e03df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, p=None, d_ff=None, vocab_size=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # Declare an embedding, a positional encoder and a transformer\n",
    "        # encoder.\n",
    "        self.enc_embedding = nn.Embedding(vocab_size, p)\n",
    "        self.enc_positional_encoding = PositionalEncoding(p)\n",
    "        self.encoder = TransformerEncoder(p=p, d_ff=d_ff)\n",
    "\n",
    "        # Declare an embedding, a positional encoder and a transformer\n",
    "        # decoder.\n",
    "        self.dec_embedding = nn.Embedding(vocab_size, p)\n",
    "        self.dec_positional_encoding = PositionalEncoding(p)\n",
    "        self.decoder = TransformerDecoder(p=p, d_ff=d_ff)\n",
    "\n",
    "        self.generator = nn.Linear(p, vocab_size)\n",
    "\n",
    "    def encode(self, X):\n",
    "        # Use `self.enc_embedding`, `self.enc_positional_encoding` and\n",
    "        # `self.encoder` to compute `Xp`\n",
    "        # <answer>\n",
    "        X_emb = self.enc_embedding(X)\n",
    "        X_emb_pos = self.enc_positional_encoding(X_emb)\n",
    "        Xp = self.encoder(X_emb_pos)\n",
    "        # </answer>\n",
    "        return Xp\n",
    "\n",
    "    def decode(self, Xp, Y):\n",
    "        # Use `self.dec_embedding`, `self.dec_positional_encoding` and\n",
    "        # `self.decoder` to compute `outs`\n",
    "        # <answer>\n",
    "        Y_emb = self.dec_embedding(Y)\n",
    "        Y_emb_pos = self.dec_positional_encoding(Y_emb)\n",
    "        outs = self.decoder(Xp, Y_emb_pos)\n",
    "        # </answer>\n",
    "        return outs\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        Xp = self.encode(X)\n",
    "        outs = self.decode(Xp, Y)\n",
    "        return self.generator(outs)\n",
    "\n",
    "\n",
    "\n",
    "def train_epoch(model: nn.Module, dataset: Dataset, optimizer: Optimizer):\n",
    "    # Training mode\n",
    "    model.train()\n",
    "\n",
    "    # Set loss function to use. Don't forget to tell the loss function to\n",
    "    # ignore entries that are padded.\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "    # Turn `dataset` into an iterable on mini-batches using `DataLoader`.\n",
    "    train_dataloader = DataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    losses = 0\n",
    "    for X, Y in train_dataloader:\n",
    "        # Select all but last element in sequences\n",
    "        Y_input = Y[:-1, : ]\n",
    "\n",
    "        # Resetting gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute output of transformer from `X` and `Y_input`.\n",
    "        scores = model.forward(X, Y_input)\n",
    "\n",
    "        # Back-propagation through loss function\n",
    "        # Select all but first element in sequences\n",
    "        Y_output = Y[1:, :]\n",
    "\n",
    "        # Compute the cross-entropy loss between `scores` and\n",
    "        # `Y_output`. `scores` is `seq_length` * `batch_size` *\n",
    "        # `vocab_size` and contains scores and `Y_output` is\n",
    "        # `seq_length` * `batch_size` and contains integers. Two ways\n",
    "        # to compute the loss:\n",
    "        #\n",
    "        # - reshape both tensors to have `batch_size` * `probs` for `scores` and\n",
    "        #   `batch_size` for `Y_output`\n",
    "        # - permute dimensions to have `batch_size` * `vocab_size` *\n",
    "        #   `seq_length` for `scores` and `batch_size` * `seq_length` for\n",
    "        #   `Y_output`\n",
    "        loss = loss_fn(scores.permute([1, 2, 0]), Y_output.transpose(0, 1))\n",
    "\n",
    "        # Gradient descent update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2703d51-156d-4b95-85ea-75de870a4a55",
   "metadata": {},
   "source": [
    "## Eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d7b8e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, val_dataset: Dataset):\n",
    "    model.eval()\n",
    "\n",
    "    # Set loss function to use. Don't forget to tell the loss function to\n",
    "    # ignore entries that are padded.\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Turn `val_dataset` into an iterable on mini-batches using `DataLoader`.\n",
    "    val_dataloader = DataLoader(dataset=dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "    losses = 0\n",
    "    for X, Y in val_dataloader:\n",
    "        # Select all but last element in sequences\n",
    "        Y_input = Y[:-1, :]\n",
    "\n",
    "        # Compute output of transformer from `X` and `Y_input`.\n",
    "        scores = model.forward(X, Y_input)\n",
    "\n",
    "        # Select all but first element in sequences\n",
    "        Y_output = Y[1:, :]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(scores.permute([1, 2, 0]), Y_output.transpose(0, 1))\n",
    "\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cab4e3-9ccd-4440-af7d-e2d597bd36a6",
   "metadata": {},
   "source": [
    "## Learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f83caa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 0.009, Val loss: 0.000, Epoch time = 1.543s\n",
      "Epoch: 2, Train loss: 0.007, Val loss: 0.001, Epoch time = 1.379s\n",
      "Epoch: 3, Train loss: 0.006, Val loss: 0.001, Epoch time = 1.347s\n",
      "Epoch: 4, Train loss: 0.006, Val loss: 0.001, Epoch time = 1.398s\n",
      "Epoch: 5, Train loss: 0.005, Val loss: 0.001, Epoch time = 1.358s\n",
      "Epoch: 6, Train loss: 0.005, Val loss: 0.001, Epoch time = 1.370s\n",
      "Epoch: 7, Train loss: 0.004, Val loss: 0.000, Epoch time = 1.404s\n",
      "Epoch: 8, Train loss: 0.004, Val loss: 0.000, Epoch time = 1.295s\n",
      "Epoch: 9, Train loss: 0.004, Val loss: 0.000, Epoch time = 1.325s\n",
      "Epoch: 10, Train loss: 0.004, Val loss: 0.000, Epoch time = 1.466s\n",
      "Epoch: 11, Train loss: 0.003, Val loss: 0.000, Epoch time = 1.283s\n",
      "Epoch: 12, Train loss: 0.003, Val loss: 0.001, Epoch time = 1.328s\n",
      "Epoch: 13, Train loss: 0.003, Val loss: 0.001, Epoch time = 1.308s\n",
      "Epoch: 14, Train loss: 0.003, Val loss: 0.001, Epoch time = 1.272s\n",
      "Epoch: 15, Train loss: 0.003, Val loss: 0.001, Epoch time = 1.286s\n",
      "Epoch: 16, Train loss: 0.003, Val loss: 0.001, Epoch time = 1.265s\n",
      "Epoch: 17, Train loss: 0.003, Val loss: 0.001, Epoch time = 1.309s\n",
      "Epoch: 18, Train loss: 0.003, Val loss: 0.000, Epoch time = 1.324s\n",
      "Epoch: 19, Train loss: 0.003, Val loss: 0.000, Epoch time = 1.310s\n",
      "Epoch: 20, Train loss: 0.002, Val loss: 0.000, Epoch time = 1.355s\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(\n",
    "    p=EMB_SIZE,\n",
    "    d_ff=FFN_HID_DIM,\n",
    "    vocab_size=VOCAB_SIZE\n",
    ")\n",
    "\n",
    "optimizer = Adam(transformer.parameters())\n",
    "\n",
    "train_set = GotoDataset(n_sequences=N_SEQUENCES)\n",
    "test_set = GotoDataset(n_sequences=N_SEQUENCES)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, train_set, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer, test_set)\n",
    "    print(\n",
    "        (\n",
    "            f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"\n",
    "            f\"Epoch time = {(end_time - start_time):.3f}s\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3b1714-05ef-446a-a1a3-5824710587ad",
   "metadata": {},
   "source": [
    "## Helpers functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "871f36ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: -5 -4 -4 -4 B C C +4 C +5 +5 -6 +6 C -5\n",
      "Output: B C C -6 C -5 C C +5\n",
      "Pred: -4 C -4 C -4 C C\n"
     ]
    }
   ],
   "source": [
    "def greedy_decode(model, src, start_symbol_idx):\n",
    "    \"\"\"Autoregressive decoding of `src` starting with `start_symbol_idx`.\"\"\"\n",
    "\n",
    "    memory = model.encode(src)\n",
    "    ys = torch.LongTensor([[start_symbol_idx]])\n",
    "    maxlen = 100\n",
    "\n",
    "    for i in range(maxlen):\n",
    "        m = ys.size(0)\n",
    "        tgt_mask = torch.triu(torch.full((m, m), float(\"-inf\")), diagonal=1)\n",
    "\n",
    "        # Decode `ys`. `out` is of size `curr_len` * 1 * `vocab_size`\n",
    "        out = model.decode(memory, ys)\n",
    "\n",
    "        # Select encoding of last token\n",
    "        enc = out[-1, 0, :]\n",
    "\n",
    "        # Get a set of scores on vocabulary\n",
    "        dist = model.generator(enc)\n",
    "\n",
    "        # Get index of maximum\n",
    "        idx = torch.argmax(dist).item()\n",
    "\n",
    "        # Add predicted index to `ys`\n",
    "        ys = torch.cat((ys, torch.LongTensor([[idx]])))\n",
    "\n",
    "        if idx == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "def translate(model: torch.nn.Module, src_sentence: Iterable):\n",
    "    \"\"\"Translate sequence `src_sentence` with `model`.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Numericalize source\n",
    "    src_tensor = torch.LongTensor([tok2idx[tok] for tok in [\"<bos>\"] + list(src_sentence) + [\"<eos>\"]])\n",
    "\n",
    "    # Fake a minibatch of size one\n",
    "    src = src_tensor.unsqueeze(1)\n",
    "\n",
    "    # Translate `src`\n",
    "    tgt_tokens = greedy_decode(model, src, BOS_IDX)\n",
    "\n",
    "    tgt_tokens = tgt_tokens.flatten().numpy()\n",
    "    return \" \".join(idx2tok[idx] for idx in tgt_tokens[1:-1])\n",
    "\n",
    "\n",
    "input, output = dataset[0]\n",
    "\n",
    "print(\"Input:\", \" \".join(input))\n",
    "print(\"Output:\", \" \".join(output))\n",
    "print(\"Pred:\", translate(transformer, input))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
