{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bf8ba06-2f7b-44a3-aa1e-dd916a2874a8",
   "metadata": {},
   "source": [
    "# Image Style Transfer Using Convolutional Neural Networks\n",
    "\n",
    "This notebook implements the algorithm found in [(Gatys\n",
    "2016)](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c20059cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as utils\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Size of images given to the network. You can reduce it if the learning is too\n",
    "# slow.\n",
    "imsize = 200\n",
    "loader = transforms.Compose([transforms.Resize(imsize), transforms.ToTensor()])\n",
    "\n",
    "# Load `content_img` as a torch tensor of size 3 * `imsize` * `imsize`\n",
    "image = Image.open(\"./data/images/dancing.jpg\")\n",
    "content_img = loader(image)\n",
    "\n",
    "# Load `style_img` as a torch tensor of size 3 * `imsize` * `imsize`\n",
    "image = Image.open(\"./data/images/picasso.jpg\")\n",
    "style_img = loader(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9327b10-7865-4ecb-86e2-d3ee0064d382",
   "metadata": {},
   "source": [
    "## Feature extraction with VGG19\n",
    "\n",
    "The next cell is a model that extracts convolutional features specified\n",
    "by `modules_indexes` from a VGG19 pretrained model. It is used to\n",
    "compute the features of the content and style image. It is also used to\n",
    "reconstruct the target image by backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db05076",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19Features(nn.Module):\n",
    "    def __init__(self, modules_indexes=[]):\n",
    "        super(VGG19Features, self).__init__()\n",
    "\n",
    "        # VGG19 pretrained model in evaluation mode\n",
    "        self.vgg19 = models.vgg19(weights=models.VGG19_Weights.DEFAULT).eval()\n",
    "\n",
    "        # Indexes of convolutional layers to remember during forward phase\n",
    "        self.modules_indexes = modules_indexes\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Define a hardcoded `mean` and `std` of size 3 * 1 * 1\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
    "\n",
    "        # First center and normalize `input` with `mean` and `std`\n",
    "        # <answer>\n",
    "        input_norm = (input - mean) / std\n",
    "        # </answer>\n",
    "\n",
    "        # Add a fake mini-batch dimension to `input_norm`\n",
    "        # <answer>\n",
    "        input_norm = input_norm.unsqueeze(0)\n",
    "        # </answer>\n",
    "\n",
    "        # Install hooks on specified modules to save their features\n",
    "        features = []\n",
    "        handles = []\n",
    "        for module_index in self.modules_indexes:\n",
    "\n",
    "            def hook(module, input, output):\n",
    "                # `output` is of size (`batchsize` = 1) * `n_filters`\n",
    "                # * `imsize` * `imsize`\n",
    "                features.append(output)\n",
    "\n",
    "            handle = self.vgg19.features[module_index].register_forward_hook(hook)\n",
    "            handles.append(handle)\n",
    "\n",
    "        # Forward propagate `input_norm`. This will trigger the hooks\n",
    "        # set up above and populate `features`\n",
    "        self.vgg19(input_norm)\n",
    "\n",
    "        # Remove hooks\n",
    "        [handle.remove() for handle in handles]\n",
    "\n",
    "        # The output of our custom VGG19Features neural network is a\n",
    "        # list of features of `input`\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c92a1c6-c2ae-4ab2-931a-ba6dfd2f65c6",
   "metadata": {},
   "source": [
    "The next cell defines the convolutional layers we will use to capture\n",
    "the style and the content. From the paper, the layers `conv1_1`,\n",
    "`conv2_1`, `conv3_1`, `conv4_1`, `conv4_2` and `conv5_1` are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e301267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of convolutional layers to extract\n",
    "modules_names = [\"conv1_1\", \"conv2_1\", \"conv3_1\", \"conv4_1\", \"conv4_2\", \"conv5_1\"]\n",
    "\n",
    "# To have access to them we need to specify their corresponding index in the\n",
    "# vgg19 network. Print VGG19Features().vgg19.features.\n",
    "\n",
    "# <answer>\n",
    "modules_indexes = [0, 5, 10, 19, 21, 28]\n",
    "# </answer>\n",
    "\n",
    "# Define out feature extractor model\n",
    "vgg19_feat = VGG19Features(modules_indexes)\n",
    "\n",
    "# Extract features of the content image\n",
    "content_features = [f.detach() for f in vgg19_feat.forward(content_img)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff3de30-31f1-427f-b99c-105d6c7a4eb5",
   "metadata": {},
   "source": [
    "## Style features as gram matrix of convolutional features\n",
    "\n",
    "The next cell computes the gram matrix of `input`. We first need to\n",
    "reshape `input` before computing the gram matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7738455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input):\n",
    "    batchsize, n_filters, width, height = input.size()\n",
    "\n",
    "    # Reshape `input` into `n_filters` * `n_pixels`\n",
    "    # <answer>\n",
    "    features = input.view(n_filters, width * height)\n",
    "    # </answer>\n",
    "\n",
    "    # Compute the inner products between filters in `G`\n",
    "    # <answer>\n",
    "    G = torch.mm(features, features.t())\n",
    "    # </answer>\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "# Extract gram features of the style image\n",
    "style_gram_features = [gram_matrix(f.detach()) for f in vgg19_feat.forward(style_img)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7fee69-ed4f-456b-b19d-8796d134fa55",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "Look at the paper to see what is the algorithm they are using. Remember\n",
    "that we are optimizing on a target image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a85a1a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = content_img.clone().requires_grad_(True)\n",
    "\n",
    "# Define `optimizer` to use L-BFGS algorithm to do gradient descent on\n",
    "# `target`\n",
    "# <answer>\n",
    "optimizer = optim.LBFGS([target])\n",
    "# </answer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da532135-d5f5-4752-90c3-6a24851b4ccc",
   "metadata": {},
   "source": [
    "## The algorithm\n",
    "\n",
    "From the paper, there are two different losses. The style loss and the\n",
    "content loss.\n",
    "\n",
    "Define `style_weight` the trade-off parameter between style and content\n",
    "losses $\\alpha/\\beta$ in the paper). It depends on chosen content and\n",
    "style images. A good choice for `dancing.jpg` and `picasso.jpg` is\n",
    "$1e-5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8e7b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <answer>\n",
    "content_weight = 1e-5\n",
    "# </answer>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a5e9200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0:\n",
      "Style Loss: 2.237663 Content Loss: 95853.664062 Overall: 3.196200\n"
     ]
    }
   ],
   "source": [
    "for step in range(1):\n",
    "    # To keep track of the losses in the closure\n",
    "    losses = {}\n",
    "\n",
    "    # Need to use a closure that computes the loss and gradients to allow the\n",
    "    # optimizer to evaluate repeatedly at different locations\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # First, forward propagate `target` through `vgg19_feat` and store its\n",
    "        # output as `target_features`.\n",
    "        # <answer>\n",
    "        target_features = vgg19_feat(target)\n",
    "        # </answer>\n",
    "\n",
    "        # Define `content_loss` on layer \"conv4_2\" (equation (1))\n",
    "        conv4_2_index = modules_names.index(\"conv4_2\")\n",
    "        # <answer>\n",
    "        content_loss = 0.5 * torch.sum(\n",
    "            (target_features[conv4_2_index] - content_features[conv4_2_index]) ** 2\n",
    "        )\n",
    "        # </answer>\n",
    "\n",
    "        # Define `style_loss` (equations (4) and (5))\n",
    "        style_loss = 0\n",
    "        for target_feature, style_gram_feature, module_name in zip(\n",
    "            target_features, style_gram_features, modules_names\n",
    "        ):\n",
    "            if module_name not in [\n",
    "                \"conv1_1\",\n",
    "                \"conv2_1\",\n",
    "                \"conv3_1\",\n",
    "                \"conv4_1\",\n",
    "                \"conv5_1\",\n",
    "            ]:\n",
    "                continue\n",
    "\n",
    "            # Compute Gram matrix\n",
    "            # <answer>\n",
    "            target_gram_feature = gram_matrix(target_feature)\n",
    "            # </answer>\n",
    "\n",
    "            # Compute style loss for current feature\n",
    "            # <answer>\n",
    "            current_style_loss = (\n",
    "                1\n",
    "                / 4\n",
    "                * torch.sum((target_gram_feature - style_gram_feature) ** 2)\n",
    "                / target_gram_feature.numel() ** 2\n",
    "            )\n",
    "            # </answer>\n",
    "\n",
    "            # Add current loss to `style_loss`\n",
    "            # <answer>\n",
    "            style_loss += 1 / 5 * current_style_loss\n",
    "            # </answer>\n",
    "\n",
    "        # Compute combined loss (equation (7))\n",
    "        # <answer>\n",
    "        loss = content_weight * content_loss + style_loss\n",
    "        # </answer>\n",
    "\n",
    "        # Store the losses\n",
    "        losses[\"loss\"] = loss.item()\n",
    "        losses[\"style_loss\"] = style_loss.item()\n",
    "        losses[\"content_loss\"] = content_loss.item()\n",
    "\n",
    "        # Backward propagation and return loss\n",
    "        # <answer>\n",
    "        loss.backward()\n",
    "        return loss\n",
    "        # </answer>\n",
    "\n",
    "    # Gradient step : don't forget to pass the closure to the optimizer\n",
    "    # <answer>\n",
    "    optimizer.step(closure)\n",
    "    # </answer>\n",
    "\n",
    "    print(\"step {}:\".format(step))\n",
    "    print(\n",
    "        \"Style Loss: {:4f} Content Loss: {:4f} Overall: {:4f}\".format(\n",
    "            losses[\"style_loss\"], losses[\"content_loss\"], losses[\"loss\"]\n",
    "        )\n",
    "    )\n",
    "    img = target.clone().squeeze()\n",
    "    img = img.clamp_(0, 1)\n",
    "    utils.save_image(img, \"output-{:03}.png\".format(step))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
