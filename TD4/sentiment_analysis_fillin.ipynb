{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1841e513-990e-4911-8d7f-49d06d8d29e9",
   "metadata": {},
   "source": [
    "# Word embedding and RNN for sentiment analysis\n",
    "\n",
    "The goal of the following notebook is to predict whether a written\n",
    "critic about a movie is positive or negative. For that we will try three\n",
    "models. A simple linear model on the word embeddings, a recurrent neural\n",
    "network and a CNN.\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "### Libraries and Imports\n",
    "\n",
    "First some imports are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91d0410c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f932056c-486a-4b15-8a8c-239d7efb02ff",
   "metadata": {},
   "source": [
    "### Global variables\n",
    "\n",
    "First letâ€™s define a few variables. `EMBEDDING_DIM` is the dimension of\n",
    "the vector space used to embed all the words of the vocabulary.\n",
    "`SEQ_LENGTH` is the maximum length of a sequence, `BATCH_SIZE` is the\n",
    "size of the batches used in stochastic optimization algorithms and\n",
    "`NUM_EPOCHS` the number of times we are going thought the entire\n",
    "training set during the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbd9c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 8\n",
    "SEQ_LENGTH = 64\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b1b052-e780-4fc0-8028-c1692f41de89",
   "metadata": {},
   "source": [
    "## The `IMDb` dataset\n",
    "\n",
    "We use the `datasets` library to load the `IMDb` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd1d7053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "train_set = dataset['train']\n",
    "test_set = dataset['test']\n",
    "\n",
    "train_set[0]\n",
    "\n",
    "print(f\"Number of training examples: {len(train_set)}\")\n",
    "print(f\"Number of testing examples: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2005948c-8adc-4276-aad0-1f2144b890ce",
   "metadata": {},
   "source": [
    "### Building a vocabulary out of `IMDb` from a tokenizer\n",
    "\n",
    "We first need a tokenizer that takes and text a returns a list of tokens.\n",
    "There are many tokenizers available from other libraries. Here we use\n",
    "the `tokenizers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b946371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a word-level tokenizer in lower case\n",
    "tokenizer = Tokenizer(models.WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer.normalizer = normalizers.Lowercase()\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653fc5d6-7e4a-4b9e-93b5-7131c2fcf60b",
   "metadata": {},
   "source": [
    "Then we need to define the set of words that will be understood by the\n",
    "model: this is the vocabulary. We build it from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4171090c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "988"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts = train_set['text']\n",
    "test_texts = test_set['text']\n",
    "\n",
    "trainer = trainers.WordLevelTrainer(vocab_size=10000, special_tokens=[\"[UNK]\", \"[PAD]\"])\n",
    "tokenizer.train_from_iterator(train_texts, trainer)\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "UNK_IDX, PAD_IDX = vocab[\"[UNK]\"], vocab[\"[PAD]\"]\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "tokenizer.encode(\"All your base are belong to us\").tokens\n",
    "tokenizer.encode(\"All your base are belong to us\").ids\n",
    "\n",
    "vocab['plenty']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96907946-fba1-4622-9c40-253d0340d80f",
   "metadata": {},
   "source": [
    "## The training loop\n",
    "\n",
    "The training loop is decomposed into 3 different functions:\n",
    "\n",
    "-   `train_epoch`\n",
    "-   `evaluate`\n",
    "-   `train`\n",
    "\n",
    "### Collate function\n",
    "\n",
    "The collate function maps raw samples coming from the dataset to padded\n",
    "tensors of numericalized tokens ready to be fed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc685e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: List):\n",
    "    def collate(text):\n",
    "        \"\"\"Turn a text into a tensor of integers.\"\"\"\n",
    "        ids = tokenizer.encode(text).ids[:SEQ_LENGTH]\n",
    "        return torch.LongTensor(ids)\n",
    "\n",
    "    src_batch = [collate(sample[\"text\"]) for sample in batch]\n",
    "\n",
    "    # Pad list of tensors using `pad_sequence`\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "\n",
    "    # Define the labels tensor\n",
    "    tgt_batch = torch.Tensor([sample[\"label\"] for sample in batch])\n",
    "\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542b4fce-62dd-4900-a4a1-49acbb902361",
   "metadata": {},
   "source": [
    "### The `accuracy` function\n",
    "\n",
    "We need to implement an accuracy function to be used in the\n",
    "`train_epoch` function (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7908f1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    # `predictions` and `labels` are both tensors of same length\n",
    "\n",
    "    # Implement accuracy\n",
    "    return torch.sum((torch.sigmoid(predictions) > 0.5).float() == (labels > 0.5)).item() / len(predictions)\n",
    "\n",
    "assert accuracy(torch.Tensor([1, -2, 3]), torch.Tensor([1, 0, 1])) == 1\n",
    "assert accuracy(torch.Tensor([1, -2, -3]), torch.Tensor([1, 0, 1])) == 2 / 3\n",
    "\n",
    "\n",
    "### The `train_epoch` function\n",
    "\n",
    "def train_epoch(model: nn.Module, optimizer: Optimizer):\n",
    "    #model.to(device)\n",
    "\n",
    "    # Training mode\n",
    "    model.train()\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_set, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True\n",
    "    )\n",
    "\n",
    "    matches = 0\n",
    "    losses = 0\n",
    "    for sequences, labels in train_dataloader:\n",
    "        #sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "        # Implement a step of the algorithm:\n",
    "        #\n",
    "        # - set gradients to zero\n",
    "        # - forward propagate examples in `batch`\n",
    "        # - compute `loss` with chosen criterion\n",
    "        # - back-propagate gradients\n",
    "        # - gradient step\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(sequences)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = accuracy(predictions, labels)\n",
    "        matches += len(predictions) * acc\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_set), matches / len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e2da4e-2e6d-4b50-a30a-362c7453499b",
   "metadata": {},
   "source": [
    "### The `evaluate` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "338aee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module):\n",
    "    #model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        test_set, batch_size=BATCH_SIZE, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    losses = 0\n",
    "    matches = 0\n",
    "    for sequences, labels in val_dataloader:\n",
    "        #sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "        predictions = model(sequences)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        acc = accuracy(predictions, labels)\n",
    "        matches += len(predictions) * acc\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(test_set), matches / len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f866ff-3018-466f-b9a4-3bde8bd40ebc",
   "metadata": {},
   "source": [
    "### The `train` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da2e19ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer):\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        start_time = timer()\n",
    "        train_loss, train_acc = train_epoch(model, optimizer)\n",
    "        end_time = timer()\n",
    "        val_loss, val_acc = evaluate(model)\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, \"\n",
    "            f\"Train loss: {train_loss:.3f}, \"\n",
    "            f\"Train acc: {train_acc:.3f}, \"\n",
    "            f\"Val loss: {val_loss:.3f}, \"\n",
    "            f\"Val acc: {val_acc:.3f}, \"\n",
    "            f\"Epoch time = {(end_time - start_time):.3f}s\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfea421-ba61-4e91-8408-a6eded57bb55",
   "metadata": {},
   "source": [
    "### Helper function to predict from a character string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b12f9a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, sentence):\n",
    "    \"Predict sentiment of given sentence according to model\"\n",
    "\n",
    "    tensor, _ = collate_fn([(\"dummy\", sentence)])\n",
    "    prediction = model(tensor)\n",
    "    pred = torch.sigmoid(prediction)\n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1cc36d-d88d-4da6-9cfe-d37e50747d26",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "### Training a linear classifier with an embedding\n",
    "\n",
    "We first test a simple linear classifier on the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50f01927",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, seq_length):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Define an embedding of `vocab_size` words into a vector space\n",
    "        # of dimension `embedding_dim`.\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "\n",
    "        # Define a linear layer from dimension `seq_length` *\n",
    "        # `embedding_dim` to 1.\n",
    "        self.l1 = nn.Linear(self.seq_length * self.embedding_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # `x` is of size `seq_length` * `batch_size`\n",
    "\n",
    "        # Compute the embedding `embedded` of the batch `x`. `embedded` is\n",
    "        # of size `seq_length` * `batch_size` * `embedding_dim`\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Flatten the embedded words and feed it to the linear layer. `flatten`\n",
    "        # must be of size `batch_size` * (`seq_length` * `embedding_dim`). You\n",
    "        # might need to use `permute` first.\n",
    "        flatten = embedded.permute(1, 0, 2).reshape(-1, self.seq_length * self.embedding_dim)\n",
    "\n",
    "        # Apply the linear layer and return a squeezed version\n",
    "        # `l1` is of size `batch_size`\n",
    "        return self.l1(flatten).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9db6f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80513\n",
      "Epoch: 1, Train loss: 0.001, Train acc: 0.500, Val loss: 0.001, Val acc: 0.504, Epoch time = 9.691s\n",
      "Epoch: 2, Train loss: 0.001, Train acc: 0.533, Val loss: 0.001, Val acc: 0.512, Epoch time = 16.191s\n",
      "Epoch: 3, Train loss: 0.001, Train acc: 0.556, Val loss: 0.001, Val acc: 0.519, Epoch time = 18.647s\n",
      "Epoch: 4, Train loss: 0.001, Train acc: 0.571, Val loss: 0.001, Val acc: 0.527, Epoch time = 16.734s\n",
      "Epoch: 5, Train loss: 0.001, Train acc: 0.585, Val loss: 0.001, Val acc: 0.540, Epoch time = 16.241s\n",
      "Epoch: 6, Train loss: 0.001, Train acc: 0.595, Val loss: 0.001, Val acc: 0.552, Epoch time = 16.997s\n",
      "Epoch: 7, Train loss: 0.001, Train acc: 0.612, Val loss: 0.001, Val acc: 0.568, Epoch time = 17.188s\n",
      "Epoch: 8, Train loss: 0.001, Train acc: 0.628, Val loss: 0.001, Val acc: 0.582, Epoch time = 15.599s\n",
      "Epoch: 9, Train loss: 0.001, Train acc: 0.642, Val loss: 0.001, Val acc: 0.597, Epoch time = 16.189s\n",
      "Epoch: 10, Train loss: 0.001, Train acc: 0.658, Val loss: 0.001, Val acc: 0.612, Epoch time = 16.266s\n"
     ]
    }
   ],
   "source": [
    "embedding_net = EmbeddingNet(VOCAB_SIZE, EMBEDDING_DIM, SEQ_LENGTH)\n",
    "print(sum(torch.numel(e) for e in embedding_net.parameters()))\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "\n",
    "optimizer = Adam(embedding_net.parameters())\n",
    "train(embedding_net, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb87ce4d-c907-4757-aa1d-d12e49e33a7f",
   "metadata": {},
   "source": [
    "### Training a linear classifier with a pretrained embedding\n",
    "\n",
    "Load a GloVe pretrained embedding instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1769ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "glove_vectors = gensim.downloader.load('glove-twitter-25')\n",
    "\n",
    "# Calculate the mean vector manually\n",
    "unknown_vector = np.mean(glove_vectors.vectors, axis=0)\n",
    "vocab_vectors = torch.tensor(np.stack([glove_vectors[e] if e in glove_vectors else unknown_vector for e in vocab.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd029699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251601\n",
      "Epoch: 1, Train loss: 0.001, Train acc: 0.520, Val loss: 0.001, Val acc: 0.529, Epoch time = 16.558s\n",
      "Epoch: 2, Train loss: 0.001, Train acc: 0.574, Val loss: 0.001, Val acc: 0.543, Epoch time = 16.344s\n",
      "Epoch: 3, Train loss: 0.001, Train acc: 0.593, Val loss: 0.001, Val acc: 0.549, Epoch time = 15.202s\n",
      "Epoch: 4, Train loss: 0.001, Train acc: 0.603, Val loss: 0.001, Val acc: 0.548, Epoch time = 16.315s\n",
      "Epoch: 5, Train loss: 0.001, Train acc: 0.610, Val loss: 0.001, Val acc: 0.548, Epoch time = 16.547s\n",
      "Epoch: 6, Train loss: 0.001, Train acc: 0.610, Val loss: 0.001, Val acc: 0.550, Epoch time = 16.566s\n",
      "Epoch: 7, Train loss: 0.001, Train acc: 0.616, Val loss: 0.001, Val acc: 0.549, Epoch time = 15.145s\n",
      "Epoch: 8, Train loss: 0.001, Train acc: 0.616, Val loss: 0.001, Val acc: 0.550, Epoch time = 17.041s\n",
      "Epoch: 9, Train loss: 0.001, Train acc: 0.620, Val loss: 0.001, Val acc: 0.551, Epoch time = 15.868s\n",
      "Epoch: 10, Train loss: 0.001, Train acc: 0.620, Val loss: 0.001, Val acc: 0.550, Epoch time = 15.968s\n"
     ]
    }
   ],
   "source": [
    "class GloVeEmbeddingNet(nn.Module):\n",
    "    def __init__(self, seq_length, vocab_vectors, freeze=True):\n",
    "        super().__init__()\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # Define `embedding_dim` from vocabulary and the pretrained `embedding`.\n",
    "        self.embedding_dim = vocab_vectors.size(1)\n",
    "        self.embedding = nn.Embedding.from_pretrained(vocab_vectors, freeze=freeze)\n",
    "\n",
    "        self.l1 = nn.Linear(self.seq_length * self.embedding_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Same forward as in `EmbeddingNet`\n",
    "        # `x` is of size `batch_size` * `seq_length`\n",
    "        embedded = self.embedding(x)\n",
    "        flatten = embedded.permute(1, 0, 2).reshape(-1, self.seq_length * self.embedding_dim)\n",
    "\n",
    "        # L1 is of size batch_size\n",
    "        return self.l1(flatten).squeeze()\n",
    "\n",
    "\n",
    "glove_embedding_net1 = GloVeEmbeddingNet(SEQ_LENGTH, vocab_vectors, freeze=True)\n",
    "print(sum(torch.numel(e) for e in glove_embedding_net1.parameters()))\n",
    "\n",
    "optimizer = Adam(glove_embedding_net1.parameters())\n",
    "train(glove_embedding_net1, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b79ab2a-db3c-430c-9575-715f22b0d265",
   "metadata": {},
   "source": [
    "### Fine-tuning the pretrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8536360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and don't freeze embedding weights\n",
    "glove_embedding_net2 = GloVeEmbeddingNet(SEQ_LENGTH, vocab_vectors, freeze=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75574a2c-e0d2-4d22-88da-f51b95724766",
   "metadata": {},
   "source": [
    "### Recurrent neural network with frozen pretrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28a6e6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters for RNN model: 38201\n",
      "Epoch: 1, Train loss: 0.001, Train acc: 0.513, Val loss: 0.001, Val acc: 0.515, Epoch time = 24.432s\n",
      "Epoch: 2, Train loss: 0.001, Train acc: 0.528, Val loss: 0.001, Val acc: 0.521, Epoch time = 25.827s\n",
      "Epoch: 3, Train loss: 0.001, Train acc: 0.550, Val loss: 0.001, Val acc: 0.568, Epoch time = 25.694s\n",
      "Epoch: 4, Train loss: 0.001, Train acc: 0.576, Val loss: 0.001, Val acc: 0.560, Epoch time = 26.953s\n",
      "Epoch: 5, Train loss: 0.001, Train acc: 0.586, Val loss: 0.001, Val acc: 0.585, Epoch time = 27.790s\n",
      "Epoch: 6, Train loss: 0.001, Train acc: 0.598, Val loss: 0.001, Val acc: 0.579, Epoch time = 26.066s\n",
      "Epoch: 7, Train loss: 0.001, Train acc: 0.608, Val loss: 0.001, Val acc: 0.592, Epoch time = 23.791s\n",
      "Epoch: 8, Train loss: 0.001, Train acc: 0.621, Val loss: 0.001, Val acc: 0.617, Epoch time = 26.056s\n",
      "Epoch: 9, Train loss: 0.001, Train acc: 0.634, Val loss: 0.001, Val acc: 0.629, Epoch time = 26.283s\n",
      "Epoch: 10, Train loss: 0.001, Train acc: 0.647, Val loss: 0.001, Val acc: 0.628, Epoch time = 25.480s\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_vectors, freeze=True):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        # Define pretrained embedding\n",
    "        self.embedding = nn.Embedding.from_pretrained(vocab_vectors, freeze=freeze)\n",
    "\n",
    "        # Size of input `x_t` from `embedding`\n",
    "        self.embedding_size = self.embedding.embedding_dim\n",
    "        self.input_size = self.embedding_size\n",
    "\n",
    "        # Size of hidden state `h_t`\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Define a GRU\n",
    "        self.gru = nn.GRU(input_size=self.input_size, hidden_size=self.hidden_size)\n",
    "\n",
    "        # Linear layer on last hidden state\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        # `x` is of size `seq_length` * `batch_size` and `h0` is of size 1\n",
    "        # * `batch_size` * `hidden_size`\n",
    "\n",
    "        # Define first hidden state in not provided\n",
    "        if h0 is None:\n",
    "            # Get batch and define `h0` which is of size 1 * # `batch_size` *\n",
    "            # `hidden_size` (1 extra dimension for bidirectional)\n",
    "            batch_size = x.size(1)\n",
    "            h0 = torch.zeros(1, batch_size, self.hidden_size)\n",
    "\n",
    "        # `embedded` is of size `seq_length` * `batch_size` *\n",
    "        # `embedding_dim`\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Define `output` and `hidden` returned by GRU:\n",
    "        #\n",
    "        # - `output` is of size `seq_length` * `batch_size` * `embedding_dim`\n",
    "        #   and gathers all the hidden states along the sequence.\n",
    "        # - `hidden` is of size 1 * `batch_size` * `embedding_dim` and is the\n",
    "        #   last hidden state.\n",
    "        output, hidden = self.gru(embedded, h0)\n",
    "\n",
    "        # Apply a linear layer on the last hidden state to have a score tensor\n",
    "        # of size 1 * `batch_size` * 1, and return a one-dimensional tensor of\n",
    "        # size `batch_size`.\n",
    "        return self.linear(hidden).squeeze()\n",
    "\n",
    "\n",
    "rnn = RNN(hidden_size=100, vocab_vectors=vocab_vectors)\n",
    "print(\"Number of parameters for RNN model:\", sum(torch.numel(e) for e in rnn.parameters() if e.requires_grad))\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, rnn.parameters()), lr=0.001)\n",
    "train(rnn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67981bc8-9703-42d8-a158-98b8bc72cb4b",
   "metadata": {},
   "source": [
    "### CNN based text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8304df3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30601\n",
      "30601\n",
      "Epoch: 1, Train loss: 0.001, Train acc: 0.516, Val loss: 0.001, Val acc: 0.572, Epoch time = 24.160s\n",
      "Epoch: 2, Train loss: 0.001, Train acc: 0.565, Val loss: 0.001, Val acc: 0.602, Epoch time = 23.874s\n",
      "Epoch: 3, Train loss: 0.001, Train acc: 0.589, Val loss: 0.001, Val acc: 0.605, Epoch time = 24.312s\n",
      "Epoch: 4, Train loss: 0.001, Train acc: 0.614, Val loss: 0.001, Val acc: 0.620, Epoch time = 24.264s\n",
      "Epoch: 5, Train loss: 0.001, Train acc: 0.628, Val loss: 0.001, Val acc: 0.625, Epoch time = 22.354s\n",
      "Epoch: 6, Train loss: 0.001, Train acc: 0.641, Val loss: 0.001, Val acc: 0.632, Epoch time = 22.503s\n",
      "Epoch: 7, Train loss: 0.001, Train acc: 0.650, Val loss: 0.001, Val acc: 0.644, Epoch time = 24.703s\n",
      "Epoch: 8, Train loss: 0.001, Train acc: 0.662, Val loss: 0.001, Val acc: 0.649, Epoch time = 23.930s\n",
      "Epoch: 9, Train loss: 0.001, Train acc: 0.677, Val loss: 0.001, Val acc: 0.656, Epoch time = 24.183s\n",
      "Epoch: 10, Train loss: 0.001, Train acc: 0.691, Val loss: 0.001, Val acc: 0.654, Epoch time = 24.570s\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_vectors, freeze=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(vocab_vectors, freeze=freeze)\n",
    "        self.embedding_dim = self.embedding.embedding_dim\n",
    "\n",
    "        self.conv_0 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=100, kernel_size=(3, self.embedding_dim)\n",
    "        )\n",
    "        self.conv_1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=100, kernel_size=(4, self.embedding_dim)\n",
    "        )\n",
    "        self.conv_2 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=100, kernel_size=(5, self.embedding_dim)\n",
    "        )\n",
    "        self.linear = nn.Linear(3 * 100, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input `x` is of size `seq_length` * `batch_size` and contains integers\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # The tensor `embedded` is of size `seq_length` * `batch_size` *\n",
    "        # `embedding_dim` and should be of size `batch_size` * (`n_channels`=1)\n",
    "        # * `seq_length` * `embedding_dim` for the convolutional layers. You can\n",
    "        # use `transpose` and `unsqueeze` to make the transformation.\n",
    "\n",
    "        # <answer>\n",
    "        embedded = embedded.transpose(0, 1).unsqueeze(1)\n",
    "        # </answer>\n",
    "\n",
    "        # Tensor `embedded` is now of size `batch_size` * 1 *\n",
    "        # `seq_length` * `embedding_dim` before convolution and should\n",
    "        # be of size `batch_size` * (`out_channels` = 100) *\n",
    "        # (`seq_length` - `kernel_size[0]` + 1) after convolution and\n",
    "        # squeezing.\n",
    "        # Implement the three parallel convolutions\n",
    "        # <answer>\n",
    "        conved_0 = self.conv_0(embedded).squeeze(3)\n",
    "        conved_1 = self.conv_1(embedded).squeeze(3)\n",
    "        conved_2 = self.conv_2(embedded).squeeze(3)\n",
    "        # </answer>\n",
    "\n",
    "        # Non-linearity step, we use ReLU activation\n",
    "        # <answer>\n",
    "        conved_0_relu = F.relu(conved_0)\n",
    "        conved_1_relu = F.relu(conved_1)\n",
    "        conved_2_relu = F.relu(conved_2)\n",
    "        # </answer>\n",
    "\n",
    "        # Max-pooling layer: pooling along whole sequence\n",
    "        # Implement max pooling\n",
    "        # <answer>\n",
    "        seq_len_0 = conved_0_relu.shape[2]\n",
    "        pooled_0 = F.max_pool1d(conved_0_relu, kernel_size=seq_len_0).squeeze(2)\n",
    "\n",
    "        seq_len_1 = conved_1_relu.shape[2]\n",
    "        pooled_1 = F.max_pool1d(conved_1_relu, kernel_size=seq_len_1).squeeze(2)\n",
    "\n",
    "        seq_len_2 = conved_2_relu.shape[2]\n",
    "        pooled_2 = F.max_pool1d(conved_2_relu, kernel_size=seq_len_2).squeeze(2)\n",
    "        # </answer>\n",
    "\n",
    "        # Dropout on concatenated pooled features\n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
    "\n",
    "        # Linear layer\n",
    "        return self.linear(cat).squeeze()\n",
    "\n",
    "cnn = CNN(vocab_vectors)\n",
    "optimizer = optim.Adam(cnn.parameters())\n",
    "\n",
    "print(sum(torch.numel(e) for e in cnn.parameters() if e.requires_grad))\n",
    "\n",
    "print(\n",
    "    (3 * cnn.embedding_dim + 1) * 100  # Conv1\n",
    "    + (4 * cnn.embedding_dim + 1) * 100  # Conv2\n",
    "    + (5 * cnn.embedding_dim + 1) * 100  # Conv3\n",
    "    + 3 * 100 + 1  # Linear\n",
    ")\n",
    "train(cnn, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
